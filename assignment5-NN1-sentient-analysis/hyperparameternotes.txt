To avoid overfitting in a neural network model, you can try the following hyperparameters:

    1. Number of hidden layers: Reducing the number of hidden layers can help prevent overfitting, as it reduces the 
       complexity of the model.

    2. Number of neurons per layer: Reducing the number of neurons per layer can also help prevent overfitting, as it 
       reduces the capacity of the model.

    3. Regularization: Adding regularization techniques such as L1 or L2 regularization, dropout, or early stopping can 
       help prevent overfitting by adding constraints to the model.

    4. Batch size: Using a smaller batch size can help prevent overfitting by introducing more noise into the training 
       process.

    5. Learning rate: Reducing the learning rate can help prevent overfitting by slowing down the rate at which the model 
       learns.


The dropout layer is a regularization technique that helps prevent overfitting in neural networks.

During training, the dropout layer randomly drops out (sets to zero) a fraction of the input units to the layer, with a 
specified probability. This has the effect of introducing noise into the training process and preventing the network from 
relying too heavily on any one input feature.

By randomly dropping out input units, the dropout layer forces the network to learn more robust features that are useful 
in combination with many different input features. This can help prevent overfitting by reducing the network's reliance 
on any one input feature and making it more generalizable to new data.

The dropout rate is set to 0.5, which means that each input unit to the dropout layer will be randomly dropped out 
with a probability of 0.5 during training



The kernel regularizer is a regularization technique in neural networks that adds a penalty term to the loss function
 of the model.

The penalty term is a function of the weights of the model, and is designed to encourage the weights to take on smaller 
values. This has the effect of reducing the complexity of the model and preventing overfitting by discouraging the model
 from relying too heavily on any one input feature.

There are several types of kernel regularizers that can be used in Keras, including L1 regularization, L2 regularization,
 and L1-L2 regularization.

L1 regularization adds a penalty term to the loss function that is proportional to the absolute value of the weights.
 This has the effect of encouraging the model to learn sparse weights, where many of the weights are set to zero.

L2 regularization adds a penalty term to the loss function that is proportional to the square of the weights.
 This has the effect of encouraging the model to learn small weights, where the weights are spread out more evenly
  across the input features.

L1-L2 regularization is a combination of L1 and L2 regularization, where both penalty terms are added to the loss function.
 This has the effect of encouraging the model to learn sparse and small weights at the same time.

The kernel_regularizer argument is set to regularizers.l2(0.01), which means that an L2 penalty term
 with a coefficient of 0.01 is added to the loss function for the weights of the layer.